{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6393,
     "status": "ok",
     "timestamp": 1767041570564,
     "user": {
      "displayName": "Red Lp",
      "userId": "12186890915998845396"
     },
     "user_tz": -60
    },
    "id": "ublbAZ21u8BM"
   },
   "outputs": [],
   "source": [
    "!pip -q install fiftyone\n",
    "!pip -q install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1767041570795,
     "user": {
      "displayName": "Red Lp",
      "userId": "12186890915998845396"
     },
     "user_tz": -60
    },
    "id": "Qm16jXImzg8u",
    "outputId": "53b7dfe0-8926-45c2-c665-5ac0c427fbc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mconstantin-auga\u001b[0m (\u001b[33mConscht-Sht\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEwvv_nMsmr2"
   },
   "source": [
    "1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11480,
     "status": "ok",
     "timestamp": 1767041582279,
     "user": {
      "displayName": "Red Lp",
      "userId": "12186890915998845396"
     },
     "user_tz": -60
    },
    "id": "mCkOYyGwsmr5",
    "outputId": "63ca6a19-bf0b-429f-cb53-d2fbdf5a385e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: g:\\My Drive\\Hands-on-CV-Project2\\notebooks\n",
      "Using project root: g:\\My Drive\\Hands-on-CV-Project2\n",
      "src exists: True\n",
      "ALL: 19998 {'cubes': 9999, 'spheres': 9999}\n",
      "10%: 2000 {'cubes': 1000, 'spheres': 1000}\n",
      "train: 1600 {'cubes': 810, 'spheres': 790}\n",
      "val  : 400 {'cubes': 190, 'spheres': 210}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent  # go from notebooks/ -> CLIP-Multimodal-Learning/\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"Using project root:\", project_root)\n",
    "print(\"src exists:\", (project_root / \"src\").exists())\n",
    "import pandas as pd\n",
    "from src.models import LateFusionClassifier, IntermediateFusionClassifier\n",
    "from src.training import train_task3\n",
    "\n",
    "from src.datasets import AssessmentPairs, train_val_split, AssessmentTorchDataset, stratified_subsample, class_counts\n",
    "from torch.utils.data import DataLoader\n",
    "ROOT = Path(r\"G:\\My Drive\\Hands-on-CV-Project2\\assessment\")\n",
    "\n",
    "pairs = AssessmentPairs(ROOT).load_pairs()\n",
    "pairs_10 = stratified_subsample(pairs, frac=0.10, seed=42)\n",
    "\n",
    "train_pairs, val_pairs = train_val_split(pairs_10, val_ratio=0.2, seed=42)\n",
    "\n",
    "train_ds = AssessmentTorchDataset(train_pairs)\n",
    "val_ds   = AssessmentTorchDataset(val_pairs)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"ALL:\", len(pairs), class_counts(pairs))\n",
    "print(\"10%:\", len(pairs_10), class_counts(pairs_10))\n",
    "print(\"train:\", len(train_pairs), class_counts(train_pairs))\n",
    "print(\"val  :\", len(val_pairs), class_counts(val_pairs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etBdZBjBsmr7"
   },
   "source": [
    "2. Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16361,
     "status": "ok",
     "timestamp": 1767041599360,
     "user": {
      "displayName": "Red Lp",
      "userId": "12186890915998845396"
     },
     "user_tz": -60
    },
    "id": "R63MsyJjsmr8",
    "outputId": "a317fd89-cf13-47b1-a114-7fd1baf496c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb_in_ch: 4 lidar_in_ch: 1\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "rgb_in_ch = batch[\"rgb\"].shape[1]\n",
    "lidar_in_ch = batch[\"lidar\"].shape[1]\n",
    "print(\"rgb_in_ch:\", rgb_in_ch, \"lidar_in_ch:\", lidar_in_ch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJc6LpkKsmr-"
   },
   "source": [
    "3. Run all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch from: c:\\Users\\const\\anaconda3\\Lib\\site-packages\\torch\\__init__.py\n",
      "cwd: g:\\My Drive\\Hands-on-CV-Project2\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import torch, sys, os\n",
    "print(\"torch from:\", torch.__file__)\n",
    "print(\"cwd:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 980777,
     "status": "error",
     "timestamp": 1767042580141,
     "user": {
      "displayName": "Red Lp",
      "userId": "12186890915998845396"
     },
     "user_tz": -60
    },
    "id": "G99tn9ISsmr_",
    "outputId": "bff38d8a-3947-45bb-f95a-f41b8bb00732"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">task3_late_fusion</strong> at: <a href='https://wandb.ai/Conscht-Sht/cilp-extended-assessment/runs/r6p5p5i6' target=\"_blank\">https://wandb.ai/Conscht-Sht/cilp-extended-assessment/runs/r6p5p5i6</a><br> View project at: <a href='https://wandb.ai/Conscht-Sht/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/Conscht-Sht/cilp-extended-assessment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251231_193919-r6p5p5i6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\My Drive\\Hands-on-CV-Project2\\notebooks\\wandb\\run-20251231_194610-qb70nexj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Conscht-Sht/cilp-extended-assessment/runs/qb70nexj' target=\"_blank\">task3_late_fusion</a></strong> to <a href='https://wandb.ai/Conscht-Sht/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Conscht-Sht/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/Conscht-Sht/cilp-extended-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Conscht-Sht/cilp-extended-assessment/runs/qb70nexj' target=\"_blank\">https://wandb.ai/Conscht-Sht/cilp-extended-assessment/runs/qb70nexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train 0.6969 | val 0.6933 | f1 0.322 | 100.51s | 0 MB\n",
      "epoch 02 | train 0.6012 | val 0.5545 | f1 0.685 | 94.68s | 0 MB\n",
      "epoch 03 | train 0.4857 | val 0.5873 | f1 0.702 | 99.42s | 0 MB\n",
      "epoch 04 | train 0.4556 | val 0.4452 | f1 0.777 | 97.78s | 0 MB\n",
      "epoch 05 | train 0.3392 | val 0.4262 | f1 0.821 | 96.01s | 0 MB\n",
      "epoch 06 | train 0.3674 | val 0.3495 | f1 0.853 | 114.50s | 0 MB\n",
      "epoch 07 | train 0.2286 | val 0.2458 | f1 0.909 | 107.23s | 0 MB\n",
      "epoch 08 | train 0.1739 | val 0.2601 | f1 0.906 | 107.12s | 0 MB\n",
      "epoch 09 | train 0.1514 | val 0.1648 | f1 0.950 | 107.88s | 0 MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 40\u001b[0m\n\u001b[0;32m     13\u001b[0m run \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m     14\u001b[0m     project\u001b[38;5;241m=\u001b[39mproject,\n\u001b[0;32m     15\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask3_late_fusion\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     reinit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     32\u001b[0m m \u001b[38;5;241m=\u001b[39m LateFusionClassifier(\n\u001b[0;32m     33\u001b[0m     rgb_in_ch\u001b[38;5;241m=\u001b[39mrgb_in_ch,\n\u001b[0;32m     34\u001b[0m     lidar_in_ch\u001b[38;5;241m=\u001b[39mlidar_in_ch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     38\u001b[0m )\n\u001b[1;32m---> 40\u001b[0m out \u001b[38;5;241m=\u001b[39m train_task3(m, train_loader, val_loader, device\u001b[38;5;241m=\u001b[39mdevice, epochs\u001b[38;5;241m=\u001b[39mEPOCHS, lr\u001b[38;5;241m=\u001b[39mLR, wandb_run\u001b[38;5;241m=\u001b[39mrun, ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/fusion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m run\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[0;32m     43\u001b[0m runs\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlate_fusion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: out[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss_final\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_f1_final\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msec_per_epoch_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu_mem_mb_peak\u001b[39m\u001b[38;5;124m\"\u001b[39m]}})\n",
      "File \u001b[1;32mg:\\My Drive\\Hands-on-CV-Project2\\src\\training.py:138\u001b[0m, in \u001b[0;36mtrain_task3\u001b[1;34m(model, train_loader, val_loader, device, epochs, lr, wandb_run, ckpt_path)\u001b[0m\n\u001b[0;32m    135\u001b[0m     n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    137\u001b[0m tr_loss \u001b[38;5;241m=\u001b[39m total \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 138\u001b[0m va_loss, va_f1 \u001b[38;5;241m=\u001b[39m evaluate_loss_and_f1(model, val_loader, device)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ckpt_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (best_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m va_loss \u001b[38;5;241m<\u001b[39m best_val):\n\u001b[0;32m    140\u001b[0m     best_val \u001b[38;5;241m=\u001b[39m va_loss\n",
      "File \u001b[1;32mc:\\Users\\const\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mg:\\My Drive\\Hands-on-CV-Project2\\src\\training.py:85\u001b[0m, in \u001b[0;36mevaluate_loss_and_f1\u001b[1;34m(model, loader, device)\u001b[0m\n\u001b[0;32m     82\u001b[0m total, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     83\u001b[0m ys, ps \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     86\u001b[0m     rgb, lidar, y \u001b[38;5;241m=\u001b[39m _move(batch, device)\n\u001b[0;32m     87\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(rgb, lidar)\n",
      "File \u001b[1;32mc:\\Users\\const\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\const\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\const\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\const\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\const\\anaconda3\\Lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32mc:\\Users\\const\\anaconda3\\Lib\\multiprocessing\\connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[1;32mc:\\Users\\const\\anaconda3\\Lib\\multiprocessing\\connection.py:329\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    327\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(wait([\u001b[38;5;28mself\u001b[39m], timeout))\n",
      "File \u001b[1;32mc:\\Users\\const\\anaconda3\\Lib\\multiprocessing\\connection.py:878\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    875\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    876\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 878\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m _exhaustive_wait(waithandle_to_obj\u001b[38;5;241m.\u001b[39mkeys(), timeout)\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\Users\\const\\anaconda3\\Lib\\multiprocessing\\connection.py:810\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    808\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 810\u001b[0m     res \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWaitForMultipleObjects(L, \u001b[38;5;28;01mFalse\u001b[39;00m, timeout)\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "EMB = 200\n",
    "\n",
    "runs = []\n",
    "\n",
    "project = \"cilp-extended-assessment\"\n",
    "\n",
    "# Late fusion\n",
    "run = wandb.init(\n",
    "    project=project,\n",
    "    name=\"task3_late_fusion\",\n",
    "    config={\n",
    "        \"task\": 3,\n",
    "        \"model\": \"late_fusion\",\n",
    "        \"fusion\": \"late\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"emb\": EMB,\n",
    "        \"batch_size\": train_loader.batch_size,\n",
    "        \"subset_frac\": 0.10,\n",
    "        \"rgb_in_ch\": rgb_in_ch,\n",
    "        \"lidar_in_ch\": lidar_in_ch,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "    },\n",
    "    reinit=True,\n",
    ")\n",
    "\n",
    "m = LateFusionClassifier(\n",
    "    rgb_in_ch=rgb_in_ch,\n",
    "    lidar_in_ch=lidar_in_ch,\n",
    "    emb_size=EMB,\n",
    "    num_classes=2,\n",
    "    normalize_embeddings=False,\n",
    ")\n",
    "\n",
    "out = train_task3(m, train_loader, val_loader, device=device, epochs=EPOCHS, lr=LR, wandb_run=run, ckpt_path=\"checkpoints/fusion\")\n",
    "run.finish()\n",
    "\n",
    "runs.append({\"model\": \"late_fusion\", **{k: out[k] for k in [\"val_loss_final\",\"val_f1_final\",\"params\",\"sec_per_epoch_avg\",\"gpu_mem_mb_peak\"]}})\n",
    "\n",
    "# Intermediate variants\n",
    "for fusion in [\"concat\", \"add\", \"hadamard\"]:\n",
    "    run = wandb.init(\n",
    "        project=project,\n",
    "        name=f\"task3_intermediate_{fusion}\",\n",
    "        config={\n",
    "            \"task\": 3,\n",
    "            \"model\": f\"intermediate_{fusion}\",\n",
    "            \"fusion\": \"intermediate\",\n",
    "            \"fusion_op\": fusion,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"lr\": LR,\n",
    "            \"emb\": EMB,\n",
    "            \"batch_size\": train_loader.batch_size,\n",
    "            \"subset_frac\": 0.10,\n",
    "            \"rgb_in_ch\": rgb_in_ch,\n",
    "            \"lidar_in_ch\": lidar_in_ch,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "        },\n",
    "        reinit=True,\n",
    "    )\n",
    "\n",
    "    m = IntermediateFusionClassifier(\n",
    "        fusion=fusion,\n",
    "        rgb_in_ch=rgb_in_ch,\n",
    "        lidar_in_ch=lidar_in_ch,\n",
    "        emb_size=EMB,\n",
    "        num_classes=2,\n",
    "        normalize_embeddings=False,\n",
    "    )\n",
    "\n",
    "    out = train_task3(m, train_loader, val_loader, device=device, epochs=EPOCHS, lr=LR, wandb_run=run, ckpt_path=f\"/checkpoints/task3_{fusion}.pt\")\n",
    "    \n",
    "    print(\"wandb run:\", wandb.run.name, wandb.run.id)\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "    runs.append({\"model\": f\"intermediate_{fusion}\", **{k: out[k] for k in [\"val_loss_final\",\"val_f1_final\",\"params\",\"sec_per_epoch_avg\",\"gpu_mem_mb_peak\"]}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1018804,
     "status": "aborted",
     "timestamp": 1767042580093,
     "user": {
      "displayName": "Red Lp",
      "userId": "12186890915998845396"
     },
     "user_tz": -60
    },
    "id": "lTbgMoILsmsA"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(runs)\n",
    "print(df.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aadrFEf7INoR"
   },
   "source": [
    "We can see that intermediate_add, intermediate_concat, and late_fusion basically all perform perfectly on the dataset. Intermediate_hadamard is slightly worse, with a loss of around 3% and an F1 score of 99.97%.\n",
    "\n",
    "It is expected that add and concat outperform the hadamard fusion. If we look at the data, the LiDAR dataset displays quite clean spheres and cubes, while the RGB dataset is relatively noisy. With Hadamard fusion, the noisy RGB features are multiplied with the cleaner LiDAR features, which can lead to a loss of information. This is not the case for concat or add. During late fusion, this is also less of an issue, since the fusion happens in deeper layers after each modality has already learned more robust features.\n",
    "\n",
    "We can also see that intermediate_add and intermediate_hadamard are noticeably smaller than the other fusion models and therefore use less GPU memory. Concat is slightly larger due to the doubled feature channels at the fusion point, and late_fusion is noticeably larger overall because it maintains two separate encoders throughout the network.\n",
    "\n",
    "Concat had the highest GPU memory peak, which can be attributed to the larger feature maps created by concatenation.\n",
    "\n",
    "Finally, it is worth mentioning that concat was the fastest model to converge to a perfect score, reaching a validation loss of 0 and an F1 score of 1 after around 20 epochs. Late fusion and intermediate_add required more training, converging after approximately 30 and 36 epochs, respectively. Solving the concat problem being the easiest makes sense, since this fusion strategy clearly separates the less noisy LiDAR features from the noisier RGB ones."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
